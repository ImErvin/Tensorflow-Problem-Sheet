{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Problem Sheet\n",
    "In this problem sheet I will be using keras with tensorflow to predict the species of Iris from a flowers sepal length and width and a petals length and width.\n",
    "\n",
    "The aim of this problem sheet is to get a better understanding of how tensorflow works.\n",
    "\n",
    "As mentioned above, I'm using Fisher's Iris Dataset. If you're looking for a little bit of further reading about the Iris dataset, check out my other notebook [here](https://github.com/ImErvin/JupyterPyplotNumpy-Problem-Sheet/blob/master/IrisNotebook.ipynb).\n",
    "\n",
    "**Note:** The code is written by [salmanahmad4u](https://github.com/salmanahmad4u/keras-iris/blob/master/iris_nn.py) and adapted by [Ian Mcloughlin](https://github.com/ianmcloughlin). The minor adaptation, explanation and analysis of the code is provided by me, [Ervin Mamutov](https://github.com/imervin)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Tensorflow and Keras?\n",
    "\n",
    "Tensorflow is a popular software library for dataflow programming across a range of tasks. Tensorflow is open-source and is developed by the Google Brain Team. Tensorflow is a symbolic math library and is also used for machine learning applications such as neaural networks [1]. I will be using Tensorflow's Python API but it is available for a range of languages.\n",
    "\n",
    "Keras is an open source neural network library written in Python developed by a Google engineer: Francois Chollet. Keras acts like a \"library on top of a library\" as it is capable of running on top of MXNet, Deeplearning4j, Tensorflow, CNTK or Theano. Keras takes the functionality in core Tensorflow and adds a higher-level of abstraction to it, making it easier to experiment with deep neaural networks [2].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Tensorflow model\n",
    "\n",
    "I'm using Keras so instead of importing tensorflow, I can import Keras which uses tensorflow as the backend.\n",
    "I also import additional useful libraries such as numpy for dealing with complicated arrays and csv to read the iris csv dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras as kr\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iris dataset contains 150 rows of data, the dataset I'm using is ordered. The first 50 are setosa, the next 50 are versicolor and the last 50 are virginica. Each row contains 5 different pieces of information about the flower: the sepal length, the sepal width, the petal length, the petal width and finally the iris class (e.g: setosa, virginica etc.)\n",
    "\n",
    "I can use the 'csv' library to read in the iris dataset and to store it into relevant numpy arrays to later use that data to create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the list 'iris': 150\n",
      "51st element of list 'iris': ['7.0', '3.2', '4.7', '1.4', 'Iris-versicolor']\n"
     ]
    }
   ],
   "source": [
    "# Initiate iris as a list with the conents of IRIS_dataset.csv line by line starting on the first line ([0:])\n",
    "iris = list(csv.reader(open('IRIS_dataset.csv')))[0:]\n",
    "\n",
    "# Expected to be 150\n",
    "print(\"Length of the list 'iris':\",len(iris))\n",
    "\n",
    "# Expected to be of class versicolor and first 4 indexs to be float variables.\n",
    "print(\"51st element of list 'iris':\",iris[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the iris dataset has been loaded into an array successfully, I will split the data into input and outputs.\n",
    "\n",
    "From looking at the data above, the first 4 floats look like they should be the input because they make up the class of Iris. If the first 4 elements are the inputs then the 5th element is the output (class of iris).\n",
    "\n",
    "I can use numpy to create an array of inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51st element of inputs & outputs: [ 7.   3.2  4.7  1.4] & ['Iris-versicolor']\n"
     ]
    }
   ],
   "source": [
    "# Initiate inputs as a numpy array that's a subset of iris - reading the first 4 indices\n",
    "# as floats representing the sepal length/width and petal length/width\n",
    "# [:,:4] is numoy notation for reading a 2D array and splicing it.\n",
    "# It means: \"Take all rows of iris, within each row, return the first 4 indices as floats\"\n",
    "inputs = np.array(iris)[:,:4].astype(np.float)\n",
    "\n",
    "# Initiate outputs as a numpy array that's a subset of iris - reading the last index\n",
    "# representing the iris class.\n",
    "# [:,4:] means: \"Take all rows, and within each row splice the first 4 indices and return the remaining\"\n",
    "outputs = np.array(iris)[:,4:]\n",
    "\n",
    "# Expected to be [7.0, 3.2, 4.7, 1.4] & Iris-versicolor - same as the output of the 51st element above.\n",
    "print(\"51st element of inputs & outputs:\",inputs[50],\"&\",outputs[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected values and actual values above are matching meaning that I have successfully separated the iris list into inputs and outputs arrays.\n",
    "\n",
    "Now the outputs array needs some work because it's basically an array of recurring strings i.e: \"Iris-setosa\" x 50, \"Iris-versicolor\" x 50 etc. The main problem with this is that we're working with strings and to use libraries like keras and tensorflow we need to use integers.\n",
    "\n",
    "A better representation would be to split this array into two  - one for the unique string that occurs e.g: [\"Iris-setosa\",\"Iris-versicolor\",\"Iris-virginica\"] and another for the index of where that number occurs e.g: Iris-seotsa occurs between in the range of 0..49 etc.\n",
    "\n",
    "Numpy's '.unique' function allows to do exactly that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in the array 'outputs': ['Iris-setosa' 'Iris-versicolor' 'Iris-virginica']\n",
      "Where the unique values occur in the array 'outputs':\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "Class of iris that sits on the 51st index of outputs_ints: Iris-versicolor\n"
     ]
    }
   ],
   "source": [
    "# Explanation found on - https://docs.scipy.org/doc/numpy/reference/generated/numpy.unique.html\n",
    "# Initiate output_vals to the unique values that occur in 'outputs'\n",
    "# Initiate outputs_ints to an array of the indices where the unique values sit.\n",
    "outputs_vals, outputs_ints = np.unique(outputs, return_inverse=True)\n",
    "\n",
    "# Expected to print out [\"Iris-setosa\" \"Iris-versicolor\" \"Iris-virginica\"] \n",
    "print(\"Unique values in the array 'outputs':\",outputs_vals)\n",
    "# Expected to print out something like [0 0 0 0 0 .. 1 1 1 1 1 .. 2 2 2 2 2 ..]\n",
    "print(\"Where the unique values occur in the array 'outputs':\\n%s\"%(outputs_ints))\n",
    "# Expected to be Iris-versicolor\n",
    "print(\"Class of iris that sits on the 51st index of outputs_ints:\",outputs_vals[outputs_ints[50]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have split the array into unique values and index occurrences I can start preparing for categorical cross entropy..\n",
    "\n",
    "### What is cross entropy?\n",
    "Cross-entropy is commonly used to quantify the difference between two probability distributions. Usually the \"true\" distribution (the one that your machine learning algorithm is trying to match) is expressed in terms of a one-hot distribution [3].\n",
    "\n",
    "#### What is a multi-class classification?\n",
    "A multi-class classification is a classification task with more than two classes; e.g., classify a set of images of fruits which may be oranges, apples, or pears. Multiclass classification makes the assumption that each sample is assigned to one and only one label: a fruit can be either an apple or a pear but not both at the same time [4].\n",
    "\n",
    "#### What is a multi-label classification?\n",
    "A multi-label classification assigns to each sample a set of target labels. This can be thought as predicting properties of a data-point that are not mutually exclusive, such as topics that are relevant for a document. A text might be about any of religion, politics, finance or education at the same time or none of these [4].\n",
    "\n",
    "#### Why does this matter?\n",
    "I could use a categorical or binary cross entropy - categorical tackles a multi-class problem while binary tackles a multi-label problem. The Iris problem is a multi-class problem (An Iris can either be setosa, virginica or versicolor), hence why I need to use categorical cross entropy when calculating the loss.\n",
    "\n",
    "To prepare I need to take the 'outputs_int' and turn it into a binary matrix of \"categories\" that will look something like: [1. 0. 0.], [0. 1. 0.], [0. 0. 1.]. This will later be used for the loss function.\n",
    "\n",
    "Keras comes with a utility that does exactly that. The utils.to_categorical utility converts a class vector to binary class matrix [5].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 51st element of outputs_cats: [ 0.  1.  0.]\n"
     ]
    }
   ],
   "source": [
    "# Initiate outputs_cats to an array of categories setting the number of classes to 3 \n",
    "outputs_cats = kr.utils.to_categorical(outputs_ints,  num_classes=3)\n",
    "\n",
    "# Expected to be [0. 1. 0.] \n",
    "print(\"The 51st element of outputs_cats:\", outputs_cats[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into training and testing\n",
    "\n",
    "Tensorflow works by training and testing a model. The iris dataset doesn't have a specific training set and testing set so I'll need to create my own training and testing set. I can achieve this by splitting the dataset in two and using the first half as the training set and using the other as the testing set. The problem with this is that the data is sorted.\n",
    "\n",
    "To overcome this problem I'll need to randomly shuffle the dataset indices and split the result in to two arrays.\n",
    "\n",
    "I will use Numpy's .random.permutation, which randomly permute a sequence, or return a permuted range [6] and Numpy's .array_split lets me split an array into multiple sub-arrays [7]. I could use Numpy's random.shuffle but I would not be able to keep track of the indicies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75th index of array 'indices': 14\n",
      "76th index of array 'indices': 93\n",
      "First element of the test_indices: 14\n",
      "Last element of the train_indices: 93\n",
      "Value at the 14 (aka the 75th element of 'indices') index of 'inputs': [ 5.8  4.   1.2  0.2]\n",
      "Value at the 93 (aka the 76th element of 'indices') index of 'inputs': [ 5.   2.3  3.3  1. ]\n",
      "\n",
      "Last element of inputs_train: [ 5.8  4.   1.2  0.2]\n",
      "The 14 index of inputs: [ 5.8  4.   1.2  0.2]\n",
      "Last element of outputs_train: [ 1.  0.  0.]\n",
      "The 14 index of outputs_cats: [ 1.  0.  0.]\n",
      "The 14 index of outputs ['Iris-setosa']\n",
      "\n",
      "First element of inputs_test: [ 5.   2.3  3.3  1. ]\n",
      "The 93 index of inputs: [ 5.   2.3  3.3  1. ]\n",
      "Last element of outputs_train: [ 0.  1.  0.]\n",
      "The 93 index of outputs_cats: [ 0.  1.  0.]\n",
      "The 93 index of outputs ['Iris-versicolor']\n"
     ]
    }
   ],
   "source": [
    "# Initiate indices to be a random shuffle of 0 to 149 (length of inputs) resulting in something like [3 132 56 44 ..]\n",
    "indices = np.random.permutation(len(inputs))\n",
    "\n",
    "# Initiate train_indices to the first 75 values of indices and test_indices to the other 75 values of indices.\n",
    "train_indices, test_indices = np.array_split(indices, 2)\n",
    "\n",
    "print(\"75th index of array 'indices':\", indices[74])\n",
    "print(\"76th index of array 'indices':\", indices[75])\n",
    "# Expected to be the same as the 75th index of indices\n",
    "print(\"First element of the test_indices:\", train_indices[74])\n",
    "# Expected to be the same as the 76th index of indices\n",
    "print(\"Last element of the train_indices:\", test_indices[0])\n",
    "print(\"Value at the\",indices[74],\"(aka the 75th element of 'indices') index of 'inputs':\",inputs[indices[74]])\n",
    "print(\"Value at the\",indices[75],\"(aka the 76th element of 'indices') index of 'inputs':\",inputs[indices[75]])\n",
    "print()\n",
    "# Initiate inputs_train to the values of inputs at the indices of train_indices\n",
    "# Initiate output_train to the values of outputs_cats at the indicies of train_indices\n",
    "# E.g: If train_indices is [2,5,10], it will set inputs_train to the values found at inputs[2],inputs[5] and inputs[10]\n",
    "# and likewise for outputs_train\n",
    "inputs_train, outputs_train = inputs[train_indices], outputs_cats[train_indices]\n",
    "\n",
    "# Do the same as above for the testing set.\n",
    "inputs_test,  outputs_test  = inputs[test_indices],  outputs_cats[test_indices]\n",
    "\n",
    "# Expect the value of last element of inputs_train should be the same as the value of the \n",
    "# random index (produced in indices) of inputs\n",
    "print(\"Last element of inputs_train:\", inputs_train[74])\n",
    "print(\"The\",indices[74],\"index of inputs:\", inputs[indices[74]])\n",
    "# Same expectation for the other prints below.\n",
    "print(\"Last element of outputs_train:\", outputs_train[74])\n",
    "print(\"The\",indices[74],\"index of outputs_cats:\", outputs_cats[indices[74]])\n",
    "print(\"The\",indices[74],\"index of outputs\", outputs[indices[74]])\n",
    "print()\n",
    "print(\"First element of inputs_test:\", inputs_test[0])\n",
    "print(\"The\",indices[75],\"index of inputs:\", inputs[indices[75]])\n",
    "print(\"Last element of outputs_train:\", outputs_test[0])\n",
    "print(\"The\",indices[75],\"index of outputs_cats:\", outputs_cats[indices[75]])\n",
    "print(\"The\",indices[75],\"index of outputs\", outputs[indices[75]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above outputs indicate that I shuffled the indices of inputs succesfully and split the data into training and testing sets. This is a very common method of splitting datasets into a training and testing set, using a mask of indices rather than using the actual dataset preserves the dataset and still allows me to reflect the data on the original dataset.\n",
    "\n",
    "Now that I have split my dataset into training and testing sets, the data is ready to be processed in a neural network model.\n",
    "\n",
    "### What is a Model?\n",
    "There are many definitions of a model in different contexts. In the context of machine learning (ML), a model is an alias for a trained (or yet to be trained) model which is expected to perform some intelligent stuff. For eg. A chat bot is an ML model which may comprise of a Neural Net to interpret speech and convert it into text and another statistical model to filter the keywords of the converted speech query [8].\n",
    "\n",
    "In Tensorflow a model is represented as a dataflow graph it can be something as simple as an add function e.g: tensors a and b produce output c, model that sits inbetween is M which adds a and b.\n",
    "\n",
    "The model I'm using will contain a stack of layers. Keras comes with a \"Sequential\" model which is a linear stack of layers. The syntax for this is simply keras.models.Sequential()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers in my model: 0\n"
     ]
    }
   ],
   "source": [
    "# Initiate model as a sequential keras model.\n",
    "model = kr.models.Sequential()\n",
    "\n",
    "# Expected to be 0\n",
    "print(\"Number of layers in my model:\", len(model.layers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that my model exists, I need to add layers to it.\n",
    "\n",
    "There are Dense and Activation layers.\n",
    "\n",
    "#### What is a dense and activation layer?\n",
    "A dense layer is just your regular densely-connected NN layer. This is where all the nodes live be it input/output or hidden. An activation layer applies an activation function to an output [9]. There are several activation formulas such as Sigmoiad, softmax, relu etc.\n",
    "\n",
    "#### Why would we use an activation function??\n",
    "Activation functions introduce non-linear properties to the network. This is important because linear functions(2x) can only do so much while non-linear functions(2x^2) offer a much more complex and realistic output [10]. \n",
    "\n",
    "I will be using relu on my hidden layers and softmax on my output layers.\n",
    "\n",
    "Rectified Linear Unit (Relu) function is the most popular activation function at the moment because it doesn't involve expensive operations like TanH or Sigmoid (two other hidden layer activation formulas) so it learns faster and also avoid the vanishing gradient problem [10].\n",
    "\n",
    "Softmax is used to find the probability of multi-class classifications (as mentioned above). This activation formula will return the probabilty of which Iris class the test data is most closely relatable to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers in my model: 4\n"
     ]
    }
   ],
   "source": [
    "model = kr.models.Sequential([\n",
    "    # Define input_shape as 4 (4 input nodes : sepal l/w and petal l/w) and create 12 hidden layer nodes\n",
    "    kr.layers.Dense(12, input_shape=(4,)),\n",
    "    # Set the activation formula to relu on the dense layer above\n",
    "    kr.layers.Activation(\"relu\"),\n",
    "    # Connect another dense layer of 3 output nodes (for each Iris class). As this is the last layer it is the output layer.\n",
    "    kr.layers.Dense(3),\n",
    "    # Set the activation formula to softmax on the output dense layer\n",
    "    kr.layers.Activation(\"softmax\")\n",
    "])\n",
    "\n",
    "# Expected to be 4 as I added 4 above.\n",
    "print(\"Number of layers in my model:\", len(model.layers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model now consists of 4 layers. 4 Input nodes, 12 hidden layer nodes and 3 output nodes. Here is a visual represntation of the model:\n",
    "\n",
    "![Model](https://image.prntscr.com/image/w-KbjbMzREmX5rMQ_Z2IXg.png)\n",
    "\n",
    "Now that the model is set up I have to put it to use by configuring it for training. Keras has a .compile() function that compiles the model with some configurations such as the optimiser, loss function, metrics (accuracy)and sample_weight_node [11]. Once compiled you can train the model.\n",
    "\n",
    "I will be using the adam optimising algorithm.\n",
    "\n",
    "Keras's .fit function will train the model for a fixed number of epochs. The .fit will take in the inputs training, output training, number of epochs to run through, batch size [11].\n",
    "\n",
    "It trains by taking the input(inputs_train) applying the hidden layers to create the output (prediction). It will then check this prediction against the actual value (outputs_train). It will then go back and tweak the weights a little more to get closer prediction to the actual value. It will do that step over and over again for the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "75/75 [==============================] - 2s 21ms/step - loss: 1.3879 - acc: 0.2933\n",
      "Epoch 2/100\n",
      "75/75 [==============================] - 0s 6ms/step - loss: 1.1047 - acc: 0.3733\n",
      "Epoch 3/100\n",
      "75/75 [==============================] - 0s 5ms/step - loss: 0.9399 - acc: 0.2933\n",
      "Epoch 4/100\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.7706 - acc: 0.6133\n",
      "Epoch 5/100\n",
      "75/75 [==============================] - 0s 6ms/step - loss: 0.6731 - acc: 0.6000\n",
      "Epoch 6/100\n",
      "75/75 [==============================] - 0s 5ms/step - loss: 0.6158 - acc: 0.7467\n",
      "Epoch 7/100\n",
      "75/75 [==============================] - 0s 5ms/step - loss: 0.5725 - acc: 0.7867\n",
      "Epoch 8/100\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.5405 - acc: 0.8400\n",
      "Epoch 9/100\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.5167 - acc: 0.8267\n",
      "Epoch 10/100\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.4944 - acc: 0.8267\n",
      "Epoch 11/100\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.4705 - acc: 0.8800\n",
      "Epoch 12/100\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.4542 - acc: 0.8800\n",
      "Epoch 13/100\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.4383 - acc: 0.8800\n",
      "Epoch 14/100\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.4301 - acc: 0.8933\n",
      "Epoch 15/100\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.4085 - acc: 0.9333\n",
      "Epoch 16/100\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.3910 - acc: 0.9333\n",
      "Epoch 17/100\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.3807 - acc: 0.9067\n",
      "Epoch 18/100\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.3684 - acc: 0.9067\n",
      "Epoch 19/100\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.3587 - acc: 0.9467\n",
      "Epoch 20/100\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.3428 - acc: 0.9600\n",
      "Epoch 21/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.3307 - acc: 0.9600\n",
      "Epoch 22/100\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.3212 - acc: 0.9333\n",
      "Epoch 23/100\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.3087 - acc: 0.9733\n",
      "Epoch 24/100\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.3053 - acc: 0.9467\n",
      "Epoch 25/100\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.2976 - acc: 0.9467\n",
      "Epoch 26/100\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.2849 - acc: 0.9600- ETA: 0s - loss: 0.3869 - acc: 0.9167\n",
      "Epoch 27/100\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.2739 - acc: 0.9467\n",
      "Epoch 28/100\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.2739 - acc: 0.9600\n",
      "Epoch 29/100\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.2546 - acc: 0.9733\n",
      "Epoch 30/100\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.2578 - acc: 0.9467\n",
      "Epoch 31/100\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.2453 - acc: 0.9600\n",
      "Epoch 32/100\n",
      "75/75 [==============================] - 0s 5ms/step - loss: 0.2348 - acc: 0.9333\n",
      "Epoch 33/100\n",
      "75/75 [==============================] - 0s 5ms/step - loss: 0.2314 - acc: 0.9733\n",
      "Epoch 34/100\n",
      "75/75 [==============================] - 0s 5ms/step - loss: 0.2271 - acc: 0.9600\n",
      "Epoch 35/100\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.2226 - acc: 0.9600\n",
      "Epoch 36/100\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.2195 - acc: 0.9467A: 0s - loss: 0.1539 - acc: 1.\n",
      "Epoch 37/100\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.2023 - acc: 0.9600\n",
      "Epoch 38/100\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.2080 - acc: 0.9600\n",
      "Epoch 39/100\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.1938 - acc: 0.9467\n",
      "Epoch 40/100\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.1936 - acc: 0.9600\n",
      "Epoch 41/100\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.1959 - acc: 0.9467\n",
      "Epoch 42/100\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.1923 - acc: 0.9733\n",
      "Epoch 43/100\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.1889 - acc: 0.9600\n",
      "Epoch 44/100\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.1765 - acc: 0.9600\n",
      "Epoch 45/100\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.1809 - acc: 0.9600\n",
      "Epoch 46/100\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.1671 - acc: 0.9733\n",
      "Epoch 47/100\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.1675 - acc: 0.9467\n",
      "Epoch 48/100\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.1651 - acc: 0.9600\n",
      "Epoch 49/100\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.1630 - acc: 0.9733\n",
      "Epoch 50/100\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.1540 - acc: 0.9600\n",
      "Epoch 51/100\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.1530 - acc: 0.9733\n",
      "Epoch 52/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1452 - acc: 0.9733\n",
      "Epoch 53/100\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.1439 - acc: 0.9600\n",
      "Epoch 54/100\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.1418 - acc: 0.9733\n",
      "Epoch 55/100\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.1443 - acc: 0.9600\n",
      "Epoch 56/100\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.1358 - acc: 0.9733\n",
      "Epoch 57/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1406 - acc: 0.9600\n",
      "Epoch 58/100\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.1316 - acc: 0.9733\n",
      "Epoch 59/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1346 - acc: 0.9600\n",
      "Epoch 60/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1285 - acc: 0.9467\n",
      "Epoch 61/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1325 - acc: 0.9600\n",
      "Epoch 62/100\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.1297 - acc: 0.9333\n",
      "Epoch 63/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1269 - acc: 0.9733\n",
      "Epoch 64/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1258 - acc: 0.9733\n",
      "Epoch 65/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1202 - acc: 0.9733\n",
      "Epoch 66/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1275 - acc: 0.9467\n",
      "Epoch 67/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1151 - acc: 0.9600\n",
      "Epoch 68/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1240 - acc: 0.9600\n",
      "Epoch 69/100\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.1213 - acc: 0.9733\n",
      "Epoch 70/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1124 - acc: 0.9733\n",
      "Epoch 71/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1076 - acc: 0.9733\n",
      "Epoch 72/100\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.1073 - acc: 0.9733\n",
      "Epoch 73/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1078 - acc: 0.9733\n",
      "Epoch 74/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1144 - acc: 0.9600\n",
      "Epoch 75/100\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.1075 - acc: 0.9733\n",
      "Epoch 76/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1032 - acc: 0.9600\n",
      "Epoch 77/100\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.1014 - acc: 0.9733\n",
      "Epoch 78/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1142 - acc: 0.9333\n",
      "Epoch 79/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.0967 - acc: 0.9733\n",
      "Epoch 80/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1126 - acc: 0.9733\n",
      "Epoch 81/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.0981 - acc: 0.9733\n",
      "Epoch 82/100\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.0938 - acc: 0.9733\n",
      "Epoch 83/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.0955 - acc: 0.9600\n",
      "Epoch 84/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.0974 - acc: 0.9733\n",
      "Epoch 85/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.0948 - acc: 0.9733\n",
      "Epoch 86/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 0s 2ms/step - loss: 0.0932 - acc: 0.9733\n",
      "Epoch 87/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.0907 - acc: 0.9733\n",
      "Epoch 88/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.0967 - acc: 0.9600\n",
      "Epoch 89/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.0962 - acc: 0.9600\n",
      "Epoch 90/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.0964 - acc: 0.9600\n",
      "Epoch 91/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.0883 - acc: 0.9733\n",
      "Epoch 92/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.0863 - acc: 0.9733\n",
      "Epoch 93/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.0866 - acc: 0.9733\n",
      "Epoch 94/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.0880 - acc: 0.9733\n",
      "Epoch 95/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.0841 - acc: 0.9733\n",
      "Epoch 96/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.0921 - acc: 0.9733\n",
      "Epoch 97/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.0921 - acc: 0.9733\n",
      "Epoch 98/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.0768 - acc: 0.9600\n",
      "Epoch 99/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.0862 - acc: 0.9733\n",
      "Epoch 100/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0865 - acc: 0.9733\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x271dd1155f8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure the model with adam optimiser, categorical cross entropy as the loss function and extra metric \"accuracy\"\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Fit the model with the input_train, output_tain for 100 epochs, set the batch_size to 1 to avoid the default 32 and\n",
    "# run in verbsoe mode to track the progress.\n",
    "model.fit(inputs_train, outputs_train, epochs=100, batch_size=1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, I need to test it. Keras comes with an .evaluate function that lets me feed in the test datasets for the input and outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 0s 407us/step\n",
      "\n",
      "\n",
      "Loss: 0.0937\tAccuracy: 0.9733\n"
     ]
    }
   ],
   "source": [
    "# Initiate loss to the value of the loss function and accuracy (extra metric)\n",
    "loss, accuracy = model.evaluate(inputs_test, outputs_test, verbose=1)\n",
    "\n",
    "# This will output the loss and accuracy of the test above\n",
    "print(\"\\n\\nLoss: %6.4f\\tAccuracy: %6.4f\" % (loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained and tested I can perform my own prediction using some random data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citation\n",
    "\n",
    "[1] https://en.wikipedia.org/wiki/TensorFlow\n",
    "\n",
    "[2] https://en.wikipedia.org/wiki/Keras\n",
    "\n",
    "[3] https://stackoverflow.com/questions/41990250/what-is-cross-entropy\n",
    "\n",
    "[4] http://scikit-learn.org/stable/modules/multiclass.html\n",
    "\n",
    "[5] https://keras.io/utils/\n",
    "\n",
    "[6] https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.permutation.html\n",
    "\n",
    "[7] https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.split.html\n",
    "\n",
    "[8] https://www.quora.com/In-AI-what-is-an-ML-model\n",
    "\n",
    "[9] https://keras.io/layers/core/\n",
    "\n",
    "[10] https://www.youtube.com/watch?v=-7scQpJT7uo\n",
    "\n",
    "[11] https://keras.io/models/sequential/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
