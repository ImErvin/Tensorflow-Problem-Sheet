{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Problem Sheet\n",
    "In this problem sheet I will be using keras with tensorflow to predict the species of Iris from a flowers sepal length and width and a petals length and width.\n",
    "\n",
    "The aim of this problem sheet is to get a better understanding of how tensorflow works.\n",
    "\n",
    "As mentioned above, I'm using Fisher's Iris Dataset. If you're looking for a little bit of further reading about the Iris dataset, check out my other notebook [here](https://github.com/ImErvin/JupyterPyplotNumpy-Problem-Sheet/blob/master/IrisNotebook.ipynb).\n",
    "\n",
    "**Note:** The code is written by [salmanahmad4u](https://github.com/salmanahmad4u/keras-iris/blob/master/iris_nn.py) and adapted by [Ian Mcloughlin](https://github.com/ianmcloughlin). The minor adaptation, explanation and analysis of the code is provided by me, [Ervin Mamutov](https://github.com/imervin)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Tensorflow and Keras?\n",
    "\n",
    "Tensorflow is a popular software library for dataflow programming across a range of tasks. Tensorflow is open-source and is developed by the Google Brain Team. Tensorflow is a symbolic math library and is also used for machine learning applications such as neaural networks [1]. I will be using Tensorflow's Python API but it is available for a range of languages.\n",
    "\n",
    "Keras is an open source neural network library written in Python developed by a Google engineer: Francois Chollet. Keras acts like a \"library on top of a library\" as it is capable of running on top of MXNet, Deeplearning4j, Tensorflow, CNTK or Theano. Keras takes the functionality in core Tensorflow and adds a higher-level of abstraction to it, making it easier to experiment with deep neaural networks [2].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Tensorflow model\n",
    "\n",
    "I'm using Keras so instead of importing tensorflow, I can import Keras which uses tensorflow as the backend.\n",
    "I also import additional useful libraries such as numpy for dealing with complicated arrays and csv to read the iris csv dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras as kr\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iris dataset contains 150 rows of data, the dataset I'm using is ordered. The first 50 are setosa, the next 50 are versicolor and the last 50 are virginica. Each row contains 5 different pieces of information about the flower: the sepal length, the sepal width, the petal length, the petal width and finally the iris class (e.g: setosa, virginica etc.)\n",
    "\n",
    "I can use the 'csv' library to read in the iris dataset and to store it into relevant numpy arrays to later use that data to create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the list 'iris': 150\n",
      "51st element of list 'iris': ['7.0', '3.2', '4.7', '1.4', 'Iris-versicolor']\n"
     ]
    }
   ],
   "source": [
    "# Initiate iris as a list with the conents of IRIS_dataset.csv line by line starting on the first line ([0:])\n",
    "iris = list(csv.reader(open('IRIS_dataset.csv')))[0:]\n",
    "\n",
    "# Expected to be 150\n",
    "print(\"Length of the list 'iris':\",len(iris))\n",
    "\n",
    "# Expected to be of class versicolor and first 4 indexs to be float variables.\n",
    "print(\"51st element of list 'iris':\",iris[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the iris dataset has been loaded into an array successfully, I will split the data into input and outputs.\n",
    "\n",
    "From looking at the data above, the first 4 floats look like they should be the input because they make up the class of Iris. If the first 4 elements are the inputs then the 5th element is the output (class of iris).\n",
    "\n",
    "I can use numpy to create an array of inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51st element of inputs & outputs: [ 7.   3.2  4.7  1.4] & ['Iris-versicolor']\n"
     ]
    }
   ],
   "source": [
    "# Initiate inputs as a numpy array that's a subset of iris - reading the first 4 indices\n",
    "# as floats representing the sepal length/width and petal length/width\n",
    "# [:,:4] is numoy notation for reading a 2D array and splicing it.\n",
    "# It means: \"Take all rows of iris, within each row, return the first 4 indices as floats\"\n",
    "inputs = np.array(iris)[:,:4].astype(np.float)\n",
    "\n",
    "# Initiate outputs as a numpy array that's a subset of iris - reading the last index\n",
    "# representing the iris class.\n",
    "# [:,4:] means: \"Take all rows, and within each row splice the first 4 indices and return the remaining\"\n",
    "outputs = np.array(iris)[:,4:]\n",
    "\n",
    "# Expected to be [7.0, 3.2, 4.7, 1.4] & Iris-versicolor - same as the output of the 51st element above.\n",
    "print(\"51st element of inputs & outputs:\",inputs[50],\"&\",outputs[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected values and actual values above are matching meaning that I have successfully separated the iris list into inputs and outputs arrays.\n",
    "\n",
    "Now the outputs array needs some work because it's basically an array of recurring strings i.e: \"Iris-setosa\" x 50, \"Iris-versicolor\" x 50 etc. The main problem with this is that we're working with strings and to use libraries like keras and tensorflow we need to use integers.\n",
    "\n",
    "A better representation would be to split this array into two  - one for the unique string that occurs e.g: [\"Iris-setosa\",\"Iris-versicolor\",\"Iris-virginica\"] and another for the index of where that number occurs e.g: Iris-seotsa occurs between in the range of 0..49 etc.\n",
    "\n",
    "Numpy's '.unique' function allows to do exactly that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in the array 'outputs': ['Iris-setosa' 'Iris-versicolor' 'Iris-virginica']\n",
      "Where the unique values occur in the array 'outputs':\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "Class of iris that sits on the 51st index of outputs_ints: Iris-versicolor\n"
     ]
    }
   ],
   "source": [
    "# Explanation found on - https://docs.scipy.org/doc/numpy/reference/generated/numpy.unique.html\n",
    "# Initiate output_vals to the unique values that occur in 'outputs'\n",
    "# Initiate outputs_ints to an array of the indices where the unique values sit.\n",
    "outputs_vals, outputs_ints = np.unique(outputs, return_inverse=True)\n",
    "\n",
    "# Expected to print out [\"Iris-setosa\" \"Iris-versicolor\" \"Iris-virginica\"] \n",
    "print(\"Unique values in the array 'outputs':\",outputs_vals)\n",
    "# Expected to print out something like [0 0 0 0 0 .. 1 1 1 1 1 .. 2 2 2 2 2 ..]\n",
    "print(\"Where the unique values occur in the array 'outputs':\\n%s\"%(outputs_ints))\n",
    "# Expected to be Iris-versicolor\n",
    "print(\"Class of iris that sits on the 51st index of outputs_ints:\",outputs_vals[outputs_ints[50]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have split the array into unique values and index occurrences I can start preparing for categorical cross entropy..\n",
    "\n",
    "### What is cross entropy?\n",
    "Cross-entropy is commonly used to quantify the difference between two probability distributions. Usually the \"true\" distribution (the one that your machine learning algorithm is trying to match) is expressed in terms of a one-hot distribution [3].\n",
    "\n",
    "#### What is a multi-class classification?\n",
    "A multi-class classification is a classification task with more than two classes; e.g., classify a set of images of fruits which may be oranges, apples, or pears. Multiclass classification makes the assumption that each sample is assigned to one and only one label: a fruit can be either an apple or a pear but not both at the same time [4].\n",
    "\n",
    "#### What is a multi-label classification?\n",
    "A multi-label classification assigns to each sample a set of target labels. This can be thought as predicting properties of a data-point that are not mutually exclusive, such as topics that are relevant for a document. A text might be about any of religion, politics, finance or education at the same time or none of these [4].\n",
    "\n",
    "#### Why does this matter?\n",
    "I could use a categorical or binary cross entropy - categorical tackles a multi-class problem while binary tackles a multi-label problem. The Iris problem is a multi-class problem (An Iris can either be setosa, virginica or versicolor), hence why I need to use categorical cross entropy when calculating the loss.\n",
    "\n",
    "To prepare I need to take the 'outputs_int' and turn it into a binary matrix of \"categories\" that will look something like: [1. 0. 0.], [0. 1. 0.], [0. 0. 1.]. This will later be used for the loss function.\n",
    "\n",
    "Keras comes with a utility that does exactly that. The utils.to_categorical utility converts a class vector to binary class matrix [5].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 51st element of outputs_cats: [ 0.  1.  0.]\n"
     ]
    }
   ],
   "source": [
    "# Initiate outputs_cats to an array of categories setting the number of classes to 3 \n",
    "outputs_cats = kr.utils.to_categorical(outputs_ints,  num_classes=3)\n",
    "\n",
    "# Expected to be [0. 1. 0.] \n",
    "print(\"The 51st element of outputs_cats:\", outputs_cats[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into training and testing\n",
    "\n",
    "Tensorflow works by training and testing a model. The iris dataset doesn't have a specific training set and testing set so I'll need to create my own training and testing set. I can achieve this by splitting the dataset in two and using the first half as the training set and using the other as the testing set. The problem with this is that the data is sorted.\n",
    "\n",
    "To overcome this problem I'll need to randomly shuffle the dataset indices and split the result in to two arrays.\n",
    "\n",
    "Numpy's .random.permutation which randomly permute a sequence, or return a permuted range [6]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75th index of array 'indices': 56\n",
      "76th index of array 'indices': 28\n",
      "First element of the test_indices: 56\n",
      "Last element of the train_indices: 28\n",
      "\n",
      "Last element of inputs_train: [ 6.3  3.3  4.7  1.6]\n",
      "The 56 index of inputs: [ 6.3  3.3  4.7  1.6]\n",
      "Last element of outputs_train: [ 0.  1.  0.]\n",
      "The 56 index of inputs: [ 0.  1.  0.]\n",
      "\n",
      "First element of inputs_test: [ 5.2  3.4  1.4  0.2]\n",
      "The 28 index of inputs: [ 5.2  3.4  1.4  0.2]\n",
      "Last element of outputs_train: [ 0.  1.  0.]\n",
      "The 56 index of inputs: [ 0.  1.  0.]\n"
     ]
    }
   ],
   "source": [
    "# Initiate indices to be a random shuffle of 0 to 149 resulting in something like [3 132 56 44 ..]\n",
    "indices = np.random.permutation(len(inputs))\n",
    "\n",
    "# Initiate train_indices to the first 75 values of indices and test_indices to the other 75 values of indices.\n",
    "train_indices, test_indices = np.array_split(indices, 2)\n",
    "\n",
    "print(\"75th index of array 'indices':\", indices[74])\n",
    "print(\"76th index of array 'indices':\", indices[75])\n",
    "# Expected to be the same as the 75th index of indices\n",
    "print(\"First element of the test_indices:\", train_indices[74])\n",
    "# Expected to be the same as the 76th index of indices\n",
    "print(\"Last element of the train_indices:\", test_indices[0])\n",
    "print()\n",
    "# Initiate inputs_train to the values of inputs at the indices of train_indices\n",
    "# Initiate output_train to the values of outputs_cats at the indicies of train_indices\n",
    "# E.g: If train_indices is [2,5,10], it will set inputs_train to the values found at inputs[2],inputs[5] and inputs[10]\n",
    "# and likewise for outputs_train\n",
    "inputs_train, outputs_train = inputs[train_indices], outputs_cats[train_indices]\n",
    "\n",
    "# Do the same as above for the testing set.\n",
    "inputs_test,  outputs_test  = inputs[test_indices],  outputs_cats[test_indices]\n",
    "\n",
    "# Expect the value of last element of inputs_train should be the same as the value of the \n",
    "# random index (produced in indices) of inputs\n",
    "print(\"Last element of inputs_train:\", inputs_train[74])\n",
    "print(\"The\",indices[74],\"index of inputs:\", inputs[indices[74]])\n",
    "# Same expectation for outputs_train.\n",
    "print(\"Last element of outputs_train:\", outputs_train[74])\n",
    "print(\"The\",indices[74],\"index of inputs:\", outputs_cats[indices[74]])\n",
    "print()\n",
    "print(\"First element of inputs_test:\", inputs_test[0])\n",
    "print(\"The\",indices[75],\"index of inputs:\", inputs[indices[75]])\n",
    "# Same expectation for outputs_train.\n",
    "print(\"Last element of outputs_train:\", outputs_train[74])\n",
    "print(\"The\",indices[74],\"index of inputs:\", outputs_cats[indices[74]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citation\n",
    "\n",
    "[1] https://en.wikipedia.org/wiki/TensorFlow\n",
    "\n",
    "[2] https://en.wikipedia.org/wiki/Keras\n",
    "\n",
    "[3] https://stackoverflow.com/questions/41990250/what-is-cross-entropy\n",
    "\n",
    "[4] http://scikit-learn.org/stable/modules/multiclass.html\n",
    "\n",
    "[5] https://keras.io/utils/\n",
    "\n",
    "[6] https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.permutation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
